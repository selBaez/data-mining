\documentclass[conference]{IEEEtran}

\ifCLASSINFOpdf%

\else

\fi


\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage[backend=bibtex, style=ieee]{biblatex}
\usepackage{tabularx, caption}
\usepackage{hyperref}
\usepackage{tikz}

\def\layersep{2.5cm}
\tikzstyle{neuron}=[circle,fill=black!25,minimum size=17pt,inner sep=0pt]
\tikzstyle{input neuron}=[neuron, fill=green!50]
\tikzstyle{output neuron}=[neuron, fill=red!50]
\tikzstyle{hidden neuron}=[neuron, fill=blue!50]
\tikzstyle{annot}=[text width=4em, text centered]

\addbibresource{bibliography}

\begin{document}

\title{Data Mining --- Flavours of Physics}

\author{\IEEEauthorblockN{Selene Baez Santamaria (2572529) \\
		Andrea Jemmett (2573223) \\
		Dimitris Alivanistos (2578740)}

\IEEEauthorblockA{
Vrije Universiteit Amsterdam \\
Amsterdam, Netherlands}}

\maketitle


\begin{abstract}
%TODO
\end{abstract}


\IEEEpeerreviewmaketitle%


\section{Introduction}
The goal of this competition is to find undiscovered phenomena given data from
the Large Hadron Collider (LHC). It is a classification task for the variable
\textit{signal}, which can have values $0$ or $1$, given the values for other
$50$ variables.

The competition provides a Starter Kit, training and test sets, a specific
submission format and a couple check files to evaluate the submitted data. The
link for these assets is
\href{https://www.kaggle.com/c/flavours-of-physics/data}{here} This competition
was closed only five months ago, and it is still relevant in the field.

\subsection{Objectives}

\begin{itemize}
	\item Compare the results from having domain knowledge and not.
	\item Objective 2. %TODO
\end{itemize}

\subsection{Research question}
\label{sec:researchQuestions}


\subsection{Hypothesis}
\label{sec:hypothesis}
Comparable accuracy (at least as high as the winner of the Kaggle competition)
can be achieved by a system that lacks domain knowledge.

\subsection{Contribution}


\subsection{Organization}
Section~\ref{sec:related_work} explores the solutions submitted to Kaggle at the
time of competition. Section~\ref{sec:dataset} describes the dataset and the
competition regulations (such as the agreement measures to be met).
Section~\ref{sec:system_design} depicts the model we proposed to tackle the
classification task. Section~\ref{sec:experiment} refers to the experimental set
up we use to test our model, while Section~\ref{sec:results} shows the results
obtained. Finally, Section~\ref{sec:conclusion} summarizes our observations.


\section{Related Work}
\label{sec:related_work}
%TODO Describe solution from winner

% Maybe include other physics related competitions?


\section{The Dataset}
\label{sec:dataset}
As said, the dataset was offered on Kaggle for a competition organized by
institutions such as CERN and LHCb. The data have been collected directly at the
LHC during experiments with high energy particles collisions. The dataset
consists of a collection of collision events and their properties. The objective
of the Kaggle competition was to predict whether a $\tau \rightarrow 3\mu$ decay
(the one that identifies a lepton flavour) was present in the collision. From
scientists this phenomenon is supposed \emph{not} to happen, so the goal of the
competition was to discover $\tau \rightarrow 3\mu$ happening more frequently
than scientists currently expect.

\subsection{Training Data}
For training there is a labeled dataset ready to be used to train a classifier.
The label, marked as \texttt{signal} with range in ${0,1}$ where 1 identifies
signal events while 0 represents background events. Signal events have been
simulated while background events come from real data collected by the LHCb
detectors, observing collision of accelerated particles with a specific mass
range in which $\tau \rightarrow 3\mu$ can't happen.

The training dataset is given is CSV format and contains 49 features plus target
label. For a detailed description of the features see
Appendix~\ref{sec:features}

\subsection{Testing}
For this Kaggle competition the submission procedure is different from the usual
ones. The dataset comes with, besides a test set, an \textit{agreement} and a
\textit{correlation} set. Any submission has to pass the agreement and
correlation checks before being scored on the test set.

\subsubsection{Agreement Test}
\label{sec:agreement}
The agreement dataset contains real and simulated events for a much more known,
observed and understood decay: $Ds \rightarrow \varphi\pi$. The motivation for
this check is that since the training set contains simulated data (for a
phenomenon not well understood), it is possible for the classifier to reach high
performances by picking up features that are not well modeled by the simulation.
The check then requires the classifier not to expose a large discrepancy when
applied to real-world and simulated data.

For this score, we are provided with a dataset on the control channel $Ds
\rightarrow \varphi\pi$ which has the same features as the training set. This
type of decay is not present in the training data. The
\textit{Kolmogorov-Smirnov} test is used to evaluate the differences between
real and simulated data between the classifier distribution on each sample.
The Kolmogorov-Smirnov metric has to be smaller than 0.09 to pass the agreement
check.

\subsubsection{Correlation Test}
\label{sec:correlation}
This test checks whether the classifier is uncorrelated with the $\tau$ mass.
Because mass is a measured quantity, scientists don't trust it when building a
model. Correlation with mass in an artificial signal-like peak or lead to
incorrect estimations of background signals.

The \texttt{mass} column is not included in the test dataset. However, this hidden
mass information is used to perform a \textit{Cramer-von Mises} test,
iteratively comparing two distributions of a.\ predicted values from submission
for entire dataset and b.\ predicted values within a certain mass region in
rolling window fashion along the whole mass range. Getting similar distributions
for all mass sub-regions means that the classifier is not correlated with the
mass. The submission must give a Cramer-von Mises value less than 0.002 to pass
the correlation test.

\subsubsection{Test Set}
\label{sec:test-set}
The test set has the same columns that the training set has except for
\texttt{mass}, \texttt{production}, \texttt{min\_ANNmuon} and \texttt{signal}.
The data contained in this dataset consists of:
\begin{enumerate}
	\item simulated signal events for $\tau \rightarrow 3\mu$;
	\item real background events for $\tau \rightarrow 3\mu$;
	\item simulated events for the $Ds \rightarrow \varphi\pi$;
	\item real background events for $Ds \rightarrow \varphi\pi$.
\end{enumerate}
Events related to the control channel are not used for scoring, but by the
agreement check (Section~\ref{sec:agreement}). One should treat all samples as
coming from the same collision channel during classification.

\section{System Design}
\label{sec:system_design}
The biggest challenge of this classification task is due to the evaluation
requirements. Since the training dataset contains both simulated and real-world
signals, the classifier is required to pass an agreement test. The purpose of
this test is to be sure that the distribution of predictions for real-world data
is similar to the one for simulated signals (recall that the test set comprehend
both $\tau \rightarrow 3\mu$ and $Ds \rightarrow \varphi\pi$ and competitors are
required to treat them equally during classification). The agreement test is
implemented by means of the Kolmogorov-Smirnov test, for which the required
metric has to be lower than 0.09. In a very similar way the classifier is
tested for correlation with the \texttt{mass} feature. This is because in physics the
mass of a particle is not believed to be a trustworthy feature when it comes to
building models. So a classifier has to pass a correlation test implemented
through a Cramer-von Mises test which has to return a score lower or equal to
0.002.

We build our model based on the Ensemble learning set of classifiers and apply
different variations and heuristics to the basic concepts, such as bagging and
stacking. The model consists of a composition of ensemble classifiers
implemented via Neural Networks, with different topologies,
assembled together. The system could be break down into two main stages: a first
one where an ensemble of Neural Networks is trained and used to produce
predictions for all four datasets (train, test, agreement and correlation); a
second one where those predictions are stacked with the original data to form an
augmented dataset with as many new features as networks in the first stage's
ensemble (that is each data-point is enriched with predictions from the
ensemble).


\subsection{Neural Networks design}
\label{sec:NN_design}
%TODO describe topologies and MOTIVATE THEM!
Each neural network consists of 5 fully connected layers. The first 4 layers
have a \textit{PReLU} activation function and the last one has a
\textit{softmax} function. Furthermore, layers 2, 3, and 4 perform Dropout in
order to avoid overfitting of the network. The input and output connections, as
well as summary of the above characteristics, is shown in the following table:

\begin{table}
	\centering
	\begin{tabular}[H]{ l c c c c }
		\textbf{Layer} & \textbf{\# inputs} & \textbf{\# outputs} &
			\textbf{Dropout rate} & \textbf{Activation function} \\ \hline
		\textit{Layer 1} & \# features & 75 & 0\% & PReLU \\
		\textit{Layer 2} & 75 & 50 & 11\% & PReLU \\
		\textit{Layer 3} & 50 & 30 & 9\% & PReLU \\
		\textit{Layer 4} & 30 & 25 & 7\% & PReLU \\
		\textit{Layer 5} & 25 & 2 & 0\% & softmax \\
	\end{tabular}
	\caption{Specification of the feed-forward Neural Network used in the first
		stage of training the classifier.}
	\label{tab:model1}
\end{table}

We use a \textit{Cross entropy} loss function.

\subsection{Model Ensemble}
The first stage of training the full classifier consists in training an ensemble
of feed-forward Neural Networks using the specifics from Table~\ref{tab:model1}.
Training is done only on the training set using the \textit{bootstrapping}
techniques, which consists in using a different sampled (with replacement)
version of the dataset to train each network in the ensemble. Bootstrapping is a
sampling technique used to obtain an approximation of independent and
identically distributed predictions and is the first step required to implement
\textit{bagging}, a technique to reduce the variance of the predictions.

After training is complete we generate predictions for all four datasets and
store them for the next stage. Predictions on the test set are also aggregated
using the mean to produce a \textit{majority vote} classification; this is a
simple implementation of bagging and unfortunately it is not sufficient to pass
the agreement and correlation tests.

\begin{table} % TODO change me!
	\centering
	\begin{tabular}{ l c c c c }
		\textbf{Layer} & \textbf{\# inputs} & \textbf{\# outputs} &
			\textbf{Dropout rate} & \textbf{Activation function} \\ \hline
		\textit{Layer 1} & \# features & 75 & 0\% & PReLU \\
		\textit{Layer 2} & 75 & 50 & 11\% & PReLU \\
		\textit{Layer 3} & 50 & 30 & 9\% & PReLU \\
		\textit{Layer 4} & 30 & 25 & 7\% & PReLU \\
		\textit{Layer 5} & 25 & 2 & 0\% & softmax \\
	\end{tabular}
	\caption{Specification of the feed-forward network used for transfer
		learning.}
	\label{tab:model2}
\end{table}

\subsection{Transfer Learning}
In the second stage of training we create a single Neural Network with the
specifications given in Table~\ref{tab:model2} and its weights are initialized
with an epoch of cross-entropy optimization on the training data. Then an
optimization algorithm (using scipy and Powell's method) is executed that tries
to minimize a loss function that incorporates AUC on the training set,
Kolmogorov-Smirnov on agreement set and Cramer-von Mises on the correlation set
in the following way:

\begin{align}
	\mathcal{L} &= -\text{AUC} + \text{KS} + \text{CvM}
	\label{eqn:loss}
\end{align}

The network is fed with the augmented data from the previous stage (original
data plus predictions from the previous stage).  After each optimization
iteration we check whether the KS and CvM metrics are less or equal to 0.09 and
0.002 respectively and that the AUC is greater than the previous stored one. If
all those three conditions are met, the network is stored for later use (e.g.
prediction).

Finally the generated ensemble of classifiers needs to be aggregated to form a
single prediction per data point. This is done again with the bagging technique,
by taking a majority vote (simple average in this case, because prediction
probabilities are required).

\section{Experimental set up}
\label{sec:experiment}

\subsection{Implementation}
\label{sec:implementation}


\subsection{Training}


\subsection{Testing/Evaluation}



\section{Results}
\label{sec:results}

\section{Conclusion}
\label{sec:conclusion}

\clearpage
\appendix

\section{Features for training}
\label{sec:features}
Follows a list of available features for training:
\begin{itemize}
	\item \texttt{FlightDistance}~-- distance between $\tau$ and PV (primary vertex, the
	original protons collision point);
	\item \texttt{FlightDistanceError}~-- error on FlightDistance;
	\item \texttt{mass}~-- reconstructed $\tau$ candidate invariant mass, which
	is \textit{absent in the test samples};
	\item \texttt{LifeTime}~-- life time of tau candidate;
	\item \texttt{IP}~-- Impact Parameter of tau candidate;
	\item \texttt{IPSig}~-- Significance of Impact Parameter;
	\item \texttt{VertexChi2}~-- $\chi^2$ of $\tau$ vertex;
	\item \texttt{dira}~-- cosine of the angle between the $\tau$ momentum and line
	between PV and tau vertex;
	\item \texttt{pt}~-- transverse momentum of $\tau$;
	\item \texttt{DOCAone}~-- Distance of Closest Approach between p0 and p1;
	\item \texttt{DOCAtwo}~-- Distance of Closest Approach between p1 and p2;
	\item \texttt{DOCAthree}~-- Distance of Closest Approach between p0 and p2;
	\item \texttt{IP\_p0p2}~-- Impact parameter of the p0 and p2 pair;
	\item \texttt{IP\_p1p2}~-- Impact parameter of the p1 and p2 pair;
	\item \texttt{isolationa}~-- track isolation variable;
	\item \texttt{isolationb}~-- track isolation variable;
	\item \texttt{isolationc}~-- track isolation variable;
	\item \texttt{isolationd}~-- track isolation variable;
	\item \texttt{isolatione}~-- track isolation variable;
	\item \texttt{isolationf}~-- track isolation variable;
	\item \texttt{iso}~-- track isolation variable;
	\item \texttt{CDF1}~-- cone isolation variable;
	\item \texttt{CDF2}~-- cone isolation variable;
	\item \texttt{CDF3}~-- cone isolation variable;
	\item \texttt{production}~-- source of $\tau$ (\textit{absent from test data});
	\item \texttt{ISO\_SumBDT}~-- track isolation variable;
	\item \texttt{p0\_IsoBDT}~-- track isolation variable;
	\item \texttt{p1\_IsoBDT}~-- track isolation variable;
	\item \texttt{p2\_IsoBDT}~-- track isolation variable;
	\item \texttt{p0\_track\_Chi2Dof}~-- quality of p0 muon track;
	\item \texttt{p1\_track\_Chi2Dof}~-- quality of p1 muon track;
	\item \texttt{p2\_track\_Chi2Dof}~-- quality of p2 muon track;
	\item \texttt{p0\_pt}~-- transverse momentum of p0 muon;
	\item \texttt{p0\_p}~-- momentum of p0 muon;
	\item \texttt{p0\_eta}~-- pseudorapidity of p0 muon;
	\item \texttt{p0\_IP}~-- Impact parameter of p0 muon;
	\item \texttt{p0\_IPSig}~-- Impact Parameter Significance of p0 muon;
	\item \texttt{p1\_pt}~-- transverse momentum of p1 muon;
	\item \texttt{p1\_p}~-- momentum of p1 muon;
	\item \texttt{p1\_eta}~-- pseudorapidity of p1 muon;
	\item \texttt{p1\_IP}~-- Impact parameter of p1 muon;
	\item \texttt{p1\_IPSig}~-- Impact Parameter Significance of p1 muon;
	\item \texttt{p2\_pt}~-- transverse momentum of p2 muon;
	\item \texttt{p2\_p}~-- momentum of p2 muon;
	\item \texttt{p2\_eta}~-- pseudorapidity of p2 muon;
	\item \texttt{p2\_IP}~-- Impact parameter of p2 muon;
	\item \texttt{p2\_IPSig}~-- Impact Parameter Significance of p2 muon;
	\item \texttt{SPDhits}~-- number of hits in the SPD detector;
	\item \texttt{min\_ANNmuon}~-- muon identification. LHCb collaboration trains
  	Artificial Neural Networks (ANN) from informations from RICH, ECAL,
  	HCAL, Muon system to distinguish muons from other particles. This
  	variable denotes the minimum of the three muons ANN. This feature
  	should not be used for training and is \textit{absent from the test
		sets};
	\item \texttt{signal}~-- is the target variable to predict.
\end{itemize}

\end{document}
